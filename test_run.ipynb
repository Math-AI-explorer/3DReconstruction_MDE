{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "### for open3d important to have numpy<2.0.0 otherwise segmentation fault ###\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "from utils.reconstruction_metrics.depth_metrics import (\n",
    "    absrel_depth_error,\n",
    "    acc_under_threshold_depth_error,\n",
    "    rms_depth_error,\n",
    "    rms_log_depth_error,\n",
    ")\n",
    "\n",
    "from utils.reconstruction_metrics.pc_metrics import (\n",
    "    chamfer_distance_p,\n",
    "    chamfer_distance_l2_o3d,\n",
    "    f_score_pcd,\n",
    ")\n",
    "from utils.data.data_worker import (\n",
    "    load_scan,\n",
    "    write_obj,\n",
    "    write_obj_ply,\n",
    ")\n",
    "from utils.camera import (\n",
    "    CameraModule\n",
    ")\n",
    "from utils.sfm import (\n",
    "    SfmModule\n",
    ")\n",
    "from utils.data.data_processing import (\n",
    "    reorganise_scene_folder,\n",
    "    adjust_size_across_mask,\n",
    ")\n",
    "from utils.metric3d_mde import Metric3dMDE\n",
    "\n",
    "np.set_printoptions(linewidth=200)\n",
    "\n",
    "root_dir = './'\n",
    "points_folder = os.path.join(root_dir, 'points')\n",
    "scenes_folder = os.path.join(root_dir, 'posed_images')\n",
    "get_api = lambda y: list(filter(lambda x: '__' not in x, dir(y)))\n",
    "path_to_save = 'scene_reorganised'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 \\\n",
    "#     tr3d/tools/test.py \\\n",
    "#     tr3d/configs/tr3d/tr3d_scannet-3d-18class.py \\\n",
    "#     work_dirs/tr3d_scannet-3d-18class/latest.pth \\\n",
    "#     --eval \\\n",
    "#     mAP\n",
    "\n",
    "# python3 \\\n",
    "#     tr3d/tools/test.py \\\n",
    "#     tr3d/configs/tr3d/tr3d_scannet-3d-18class.py \\\n",
    "#     tr3d_scannet.pth \\\n",
    "#     --eval \\\n",
    "#     mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка сцены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Реорганизация сцены ###\n",
    "path_to_scene = os.path.join(scenes_folder, os.listdir(scenes_folder)[1])\n",
    "path_to_save = 'scene_reorganised'\n",
    "reorganise_scene_folder(path_to_scene, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./posed_images/scene0113_00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Подгон исходных изображений под размеры gt карт глубин ###\n",
    "path_to_depth_maps = os.path.join(path_to_save, 'depth_maps')\n",
    "path_to_images = os.path.join(path_to_save, 'images')\n",
    "output_path = os.path.join(path_to_save, 'images_adjusted')\n",
    "adjust_size_across_mask(\n",
    "    path_to_mask=path_to_depth_maps,\n",
    "    path_to_images=path_to_images,\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Анализ размеров изображений ###\n",
    "path_to_images = os.path.join(path_to_save, 'images')\n",
    "images = [\n",
    "    np.array(Image.open(os.path.join(path_to_images, file)))\n",
    "    for file in sorted(os.listdir(path_to_images))\n",
    "]\n",
    "images_unique_sizes = list(set([image.shape[:2] for image in images]))\n",
    "\n",
    "path_to_images_adjusted = os.path.join(path_to_save, 'images_adjusted')\n",
    "images_adjusted = [\n",
    "    np.array(Image.open(os.path.join(path_to_images_adjusted, file)))\n",
    "    for file in sorted(os.listdir(path_to_images_adjusted))\n",
    "]\n",
    "images_adjusted_unique_sizes = list(set([image.shape[:2] for image in images_adjusted]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sizes for original images 1\n",
      "--------------------------------------------------\n",
      "Unique sizes for adjusted images 1\n"
     ]
    }
   ],
   "source": [
    "# Если на весь датасет только один размер картинок\n",
    "# то обновляем параметры камеры\n",
    "print('Unique sizes for original images', len(images_unique_sizes))\n",
    "symbol = '-'\n",
    "print(f'{symbol * 50}')\n",
    "print('Unique sizes for adjusted images', len(images_adjusted_unique_sizes))\n",
    "\n",
    "if len(images_adjusted_unique_sizes) == len(images_unique_sizes):\n",
    "    path_to_camera = os.path.join(path_to_save, 'intrinsic.txt')\n",
    "    camera = CameraModule(path_to_camera)\n",
    "    camera_matrix = camera.get_camera_matrix()\n",
    "    camera_matrix_adjusted = CameraModule.get_adapted_matrix(\n",
    "        camera_matrix,\n",
    "        images_adjusted_unique_sizes[0],\n",
    "        images_unique_sizes[0]\n",
    "    )\n",
    "    camera_parameters_adjusted = CameraModule.convert_matrix_to_params(camera_matrix_adjusted)\n",
    "    camera.update_camera_intrinsic(camera_parameters_adjusted)\n",
    "    camera.save_camera_intrinsic(os.path.join(path_to_save, 'intrinsic_adjusted.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Normals and Metric depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/gribov/.cache/torch/hub/yvanyin_metric3d_main\n",
      "/home/gribov/cv_work/rec_env/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/gribov/cv_work/rec_env/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/gribov/.cache/torch/hub/yvanyin_metric3d_main/mono/model/backbones/ViT_DINO.py:248: FutureWarning: xformers.components is deprecated and is not maintained anymore. It might be removed in a future version of xFormers \n",
      "  from xformers.components.attention import ScaledDotProduct\n",
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "crop_size = 5\n",
    "metirc3d_model = Metric3dMDE(\n",
    "    path_to_camera='scene_reorganised/intrinsic_adjusted.txt',\n",
    "    crop_size=crop_size,\n",
    "    model_type='vit',\n",
    ")\n",
    "images_processed, focal_scaled = \\\n",
    "    metirc3d_model.preprocess_images(path_to_images='scene_reorganised/images_adjusted')\n",
    "name_to_depths = metirc3d_model.predict_processed(images_processed, focal_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_depths = os.path.join('scene_reorganised', 'depth_maps')\n",
    "\n",
    "scene_depths = {\n",
    "    file.split('.')[0]:np.array(Image.open(os.path.join(path_to_depths, file))) / 1000\n",
    "    for file in sorted(os.listdir(path_to_depths))\n",
    "}\n",
    "all_images = sorted(list(scene_depths.keys()))\n",
    "\n",
    "gts = np.stack([scene_depths[img][crop_size:-crop_size, crop_size:-crop_size] for img in all_images], axis=0)\n",
    "preds = np.stack([name_to_depths[img] for img in all_images], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AbsRel': 0.051, 'Delta_1.25': 0.99, 'RMS': 0.652, 'RMS_log': 0.075}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    'AbsRel': np.round(absrel_depth_error(gts, preds), 3),\n",
    "    'Delta_1.25': np.round(acc_under_threshold_depth_error(gts, preds), 3),\n",
    "    'RMS': np.round(rms_depth_error(gts, preds), 3),\n",
    "    'RMS_log': np.round(rms_log_depth_error(gts, preds), 3),\n",
    "}\n",
    "# scene_folder[0]: {'AbsRel': 0.041, 'Delta_1.25': 0.994, 'RMS': 0.504, 'RMS_log': 0.059}\n",
    "# scene_folder[1]: {'AbsRel': 0.051, 'Delta_1.25': 0.99, 'RMS': 0.652, 'RMS_log': 0.075}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bin(path):\n",
    "    with open(path, \"rb\") as fid:\n",
    "        width, height, channels = np.genfromtxt(\n",
    "            fid, delimiter=\"&\", max_rows=1, usecols=(0, 1, 2), dtype=int\n",
    "        )\n",
    "        fid.seek(0)\n",
    "        num_delimiter = 0\n",
    "        byte = fid.read(1)\n",
    "        while True:\n",
    "            if byte == b\"&\":\n",
    "                num_delimiter += 1\n",
    "                if num_delimiter >= 3:\n",
    "                    break\n",
    "            byte = fid.read(1)\n",
    "        array = np.fromfile(fid, np.float32)\n",
    "    array = array.reshape((width, height, channels), order=\"F\")\n",
    "    return np.transpose(array, (1, 0, 2)).squeeze()\n",
    "\n",
    "colmap_depths = list()\n",
    "path_to_colmap_depths = os.path.join(path_to_save, 'reconstruction', 'dense', 'stereo', 'depth_maps')\n",
    "for file in sorted(os.listdir(path_to_colmap_depths)):\n",
    "    name, way_to_get_depths = os.path.splitext(os.path.splitext(file)[0])\n",
    "    way_to_get_depths = way_to_get_depths[1:]\n",
    "    if way_to_get_depths == 'photometric':\n",
    "        continue\n",
    "    depth_map = read_bin(os.path.join(path_to_colmap_depths, file))\n",
    "    colmap_depths.append((os.path.splitext(name)[0], depth_map))\n",
    "\n",
    "colmap_depths = {n:d for n, d in colmap_depths}\n",
    "path_to_colmap_images = os.path.join(path_to_save, 'reconstruction', 'dense', 'images')\n",
    "colmap_images = {\n",
    "    os.path.splitext(n)[0]:np.array(Image.open(os.path.join(path_to_colmap_images, n)))\n",
    "    for n in sorted(os.listdir(path_to_colmap_images))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_names = set()\n",
    "for name in colmap_depths.keys():\n",
    "    colmap_d = colmap_depths[name]\n",
    "    metric_d = name_to_depths[name]\n",
    "    colmap_h, colmap_w = colmap_d.shape\n",
    "    metric_h, metric_w = metric_d.shape\n",
    "    dif_h, dif_w = colmap_h - metric_h, colmap_w - metric_w\n",
    "    unpad_h = dif_h // 2\n",
    "    unpad_w = dif_w // 2\n",
    "    if dif_h < 0 or dif_w < 0 or unpad_h == 0 or unpad_w == 0:\n",
    "        exclude_names.add(name)\n",
    "        continue\n",
    "    if dif_h % 2 == 0 and dif_h > 0:\n",
    "        colmap_d = colmap_d[unpad_h:-unpad_h, :]\n",
    "    elif dif_h > 0:\n",
    "        colmap_d = colmap_d[unpad_h+1:-unpad_h, :]\n",
    "    if dif_w % 2 == 0 and dif_w > 0:\n",
    "        colmap_d = colmap_d[:, unpad_w:-unpad_w]\n",
    "    elif dif_w > 0:\n",
    "        colmap_d = colmap_d[:, unpad_w+1:-unpad_w]\n",
    "    colmap_depths[name] = colmap_d\n",
    "\n",
    "colmap_images_to_take = set(list(colmap_depths.keys())).difference(exclude_names)\n",
    "\n",
    "colmap_depths_s = np.hstack([d.flatten() for n, d in colmap_depths.items() if n in colmap_images_to_take])\n",
    "metric_depths_s = np.hstack([d.flatten() for n, d in name_to_depths.items() if n in colmap_images_to_take])\n",
    "mask = np.logical_and(colmap_depths_s > 0, metric_depths_s > 0)\n",
    "colmap_depth_median, colmap_depth_avg = np.median(colmap_depths_s[mask]), np.mean(colmap_depths_s[mask])\n",
    "metric3d_depth_median, metric3d_depth_avg = np.median(metric_depths_s[mask]), np.mean(metric_depths_s[mask])\n",
    "\n",
    "median_depth_coef, avg_depth_coef = \\\n",
    "    metric3d_depth_median / colmap_depth_median, \\\n",
    "    metric3d_depth_avg / colmap_depth_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3964856"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_depth_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D -> 3D проектирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pose(path_to_pose: str) -> Dict[str, Tuple[float, float]]:\n",
    "    with open(path_to_pose, \"r\", encoding=\"utf-8\") as fin:\n",
    "        numbers =fin\\\n",
    "            .read()\\\n",
    "            .replace('\\n', ' ')\\\n",
    "            .strip()\\\n",
    "            .split(' ')\n",
    "        numbers = np.array(list(map(float, numbers))).reshape(4, -1)\n",
    "    R, t = numbers[:3, :3], numbers[:3, 3]\n",
    "    return R, t\n",
    "\n",
    "def load_scene_poses(path_to_poses: str) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:\n",
    "    scene_poses = dict()\n",
    "    for pose_file in sorted(os.listdir(path_to_poses)):\n",
    "        path_to_pose = os.path.join(path_to_poses, pose_file)\n",
    "        pose_name = pose_file.split('.', maxsplit=1)[0]\n",
    "        scene_poses[pose_name] = load_pose(path_to_pose)\n",
    "    return scene_poses\n",
    "\n",
    "def pixels_to_spatial_coords(rgb, depth, K, R, t):\n",
    "    # x = P * X\n",
    "    # P = K * [I|0] * C^-1\n",
    "    # C ~ 4x4 pose matrix\n",
    "    h, w = rgb.shape[:2]\n",
    "    x_coord_grid = np.arange(w)[None, :].repeat(h, axis=0)\n",
    "    y_coord_grid = np.arange(h)[:, None].repeat(w, axis=1)\n",
    "    xy_grid = np.dstack((x_coord_grid, y_coord_grid, np.ones((h, w))))\n",
    "    xy_grid = xy_grid.reshape(-1, 3).T\n",
    "\n",
    "    projection_to_cam_space = np.dot(np.linalg.inv(K), xy_grid)\n",
    "    projection_to_cam_space *= depth.flatten()[None, :]\n",
    "    \n",
    "    # projection_to_world_space = np.dot(R, projection_to_cam_space) + t[:, None]\n",
    "    # projection_to_world_space = projection_to_world_space.T.reshape(h, w, 3)\n",
    "    \n",
    "    R_cw = R.T\n",
    "    t_cw = -np.dot(R_cw, t)\n",
    "    projection_to_world_space = np.dot(R_cw, projection_to_cam_space) + t_cw[:, None]\n",
    "    projection_to_world_space = projection_to_world_space.T.reshape(h, w, 3)\n",
    "\n",
    "    return np.dstack((projection_to_world_space, rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = load_scene_poses(os.path.join(path_to_save, 'poses'))\n",
    "for key, val in poses.items():\n",
    "    R, t = val\n",
    "    poses[key] = np.vstack((np.hstack((R, t[:, None])), np.array([[0., 0., 0., 1.]])))\n",
    "\n",
    "cam_module = CameraModule(os.path.join(path_to_save, 'intrinsic_adjusted.txt'))\n",
    "cam_params = cam_module.get_camera_intrinsic()\n",
    "fx, fy = cam_params['focal_p']\n",
    "cx, cy = cam_params['optical_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "volume = o3d.pipelines.integration.ScalableTSDFVolume(\n",
    "    voxel_length=8.0 / 512.0,\n",
    "    sdf_trunc=0.1,\n",
    "    color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8\n",
    ")\n",
    "\n",
    "sfm = SfmModule(os.path.join(path_to_save, 'reconstruction', 'dense', 'sparse'))\n",
    "# sfm.reconstruction.normalize()\n",
    "path_to_images = os.path.join(path_to_save, 'images_adjusted')\n",
    "name_to_image = {\n",
    "    image.split('.')[0]:np.array(Image.open(os.path.join(path_to_images, image)))\n",
    "    for image in sorted(os.listdir(path_to_images))\n",
    "}\n",
    "camera_to_k = sfm.get_cameras_info()\n",
    "image_to_info = sfm.get_images_info()\n",
    "\n",
    "all_images = sorted(list(image_to_info.keys()))\n",
    "for i in tqdm(range(len(all_images)), leave=False):\n",
    "    name = all_images[i]\n",
    "    color, depth = \\\n",
    "        name_to_image[name], name_to_depths[name]\n",
    "    # color, depth = \\\n",
    "    #     colmap_images[name], colmap_depths[name]\n",
    "    # color, depth = \\\n",
    "    #     name_to_image[name], scene_depths[name].astype(np.float32)\n",
    "    h, w = color.shape[:2]\n",
    "    R, t = image_to_info[name]['rotation'], image_to_info[name]['translation']\n",
    "    t *= median_depth_coef\n",
    "    extrinsic = np.vstack((np.hstack((R, t[:, None])), np.array([[0.0, 0.0, 0.0, 1.0]])))\n",
    "    # extrinsic = poses[name]\n",
    "    if depth.shape != color.shape[:2]:\n",
    "        depth = np.pad(depth, ((crop_size, crop_size), (crop_size, crop_size)), constant_values=0)\n",
    "    color, depth = \\\n",
    "        o3d.cuda.pybind.geometry.Image(color), o3d.cuda.pybind.geometry.Image(depth)\n",
    "    rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        color, depth, depth_trunc=10.0, convert_rgb_to_intensity=False, depth_scale=1.0\n",
    "    )\n",
    "    cam_params = camera_to_k[image_to_info[name]['camera_id']]\n",
    "    fx, fy = cam_params['focal_p']\n",
    "    cx, cy = cam_params['optical_p']\n",
    "    camera_o3d = o3d.camera.PinholeCameraIntrinsic(w, h, fx, fy, cx, cy)\n",
    "    volume.integrate(\n",
    "        rgbd, camera_o3d,\n",
    "        extrinsic,\n",
    "        # np.linalg.inv(extrinsic),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointCloud with 480763 points.\n",
      "\u001b[1;33m[Open3D WARNING] Write Ply clamped color value to valid range\u001b[0;m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd = volume.extract_point_cloud()\n",
    "print(pcd)\n",
    "# pcd = np.hstack((np.asarray(pcd.points), (np.asarray(pcd.colors) * 255).astype(np.uint8)))\n",
    "# o3d.io.write_point_cloud('pred2.ply', pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PointCloud with 24090 points., PointCloud with 18888 points.)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_scene = o3d.io.read_point_cloud('reconstructed_point_clouds/scene_gt2.ply')\n",
    "pred_scene = o3d.io.read_point_cloud('reconstructed_point_clouds/sfm_metric3d.ply')\n",
    "\n",
    "pred_box = pred_scene.get_oriented_bounding_box()\n",
    "pred_scene.translate(-pred_box.center)\n",
    "pred_box = pred_scene.get_oriented_bounding_box()\n",
    "pred_scene.rotate(pred_box.R.T)\n",
    "\n",
    "gt_box = gt_scene.get_oriented_bounding_box()\n",
    "gt_scene.translate(-gt_box.center)\n",
    "\n",
    "save_rough_align_pcd = False\n",
    "if save_rough_align_pcd:\n",
    "    print('saving')\n",
    "    o3d.io.write_point_cloud(\"pred_scene_aligned.ply\", pred_scene)\n",
    "    o3d.io.write_point_cloud(\"gt_scene_aligned.ply\", gt_scene)\n",
    "\n",
    "downsampled = True\n",
    "if downsampled:\n",
    "    print('downsampling')\n",
    "    voxel_size=0.1 # 8.0 / 512.0 # 0.015625\n",
    "    radius = voxel_size\n",
    "    pred_scene = pred_scene.voxel_down_sample(radius)\n",
    "    gt_scene = gt_scene.voxel_down_sample(radius)\n",
    "else:\n",
    "    voxel_size = 8.0 / 512.0\n",
    "    \n",
    "pred_scene, gt_scene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpfh\n",
      "normals\n",
      "ransac\n"
     ]
    }
   ],
   "source": [
    "print('fpfh')\n",
    "radius_feature = voxel_size * 2\n",
    "gt_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    gt_scene, o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=90)\n",
    ")\n",
    "pred_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    pred_scene, o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=90)\n",
    ")\n",
    "\n",
    "print('normals')\n",
    "gt_scene.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "pred_scene.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "\n",
    "print('ransac')\n",
    "result_ransac = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "    source=pred_scene,\n",
    "    target=gt_scene,\n",
    "    source_feature=gt_fpfh,\n",
    "    target_feature=pred_fpfh,\n",
    "    mutual_filter=True,\n",
    "    max_correspondence_distance=voxel_size * 50,\n",
    "    estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "    ransac_n=20,\n",
    "    checkers = [\n",
    "        # o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(voxel_size * 100),\n",
    "        # o3d.pipelines.registration.CorrespondenceCheckerBasedOnNormal(np.pi)\n",
    "    ],\n",
    "    criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.24 -0.94 -0.24 -0.48]\n",
      " [ 0.95  0.28 -0.13  0.7 ]\n",
      " [ 0.19 -0.19  0.96 -0.79]\n",
      " [ 0.    0.    0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(result_ransac.transformation, 2))\n",
    "pred_scene.transform(result_ransac.transformation)\n",
    "# o3d.io.write_point_cloud('pred_scene_aligned.ply', pred_scene)\n",
    "# o3d.io.write_point_cloud('gt_scene_aligned.ply', gt_scene)\n",
    "np.save('ransac_transformation.npy', result_ransac.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.24043631, -0.94171712, -0.23528546, -0.482045  ],\n",
       "        [ 0.9523481 ,  0.27573163, -0.13040389,  0.70039514],\n",
       "        [ 0.18767922, -0.19271983,  0.9631384 , -0.79098249],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " PointCloud with 1356735 points.,\n",
       " PointCloud with 862025 points.,\n",
       " 0.015625)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_scene = o3d.io.read_point_cloud('reconstructed_point_clouds/scene_gt2.ply')\n",
    "pred_scene = o3d.io.read_point_cloud('reconstructed_point_clouds/sfm_metric3d.ply')\n",
    "result_ransac = np.load('ransac_transformation.npy')\n",
    "\n",
    "pred_box = pred_scene.get_oriented_bounding_box()\n",
    "pred_scene.translate(-pred_box.center)\n",
    "pred_box = pred_scene.get_oriented_bounding_box()\n",
    "pred_scene.rotate(pred_box.R.T)\n",
    "# transform according ransac icp\n",
    "pred_scene.transform(result_ransac)\n",
    "\n",
    "gt_box = gt_scene.get_oriented_bounding_box()\n",
    "gt_scene.translate(-gt_box.center)\n",
    "\n",
    "downsampled = False\n",
    "if downsampled:\n",
    "    print('downsampling')\n",
    "    voxel_size=0.1 # 8.0 / 512.0 # 0.015625\n",
    "    radius = voxel_size\n",
    "    pred_scene = pred_scene.voxel_down_sample(radius)\n",
    "    gt_scene = gt_scene.voxel_down_sample(radius)\n",
    "else:\n",
    "    voxel_size = 8.0 / 512.0\n",
    "    \n",
    "result_ransac, pred_scene, gt_scene, voxel_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimating normals\n",
      "icp\n"
     ]
    }
   ],
   "source": [
    "print('estimating normals')\n",
    "pred_scene.estimate_normals(\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=50)\n",
    ")\n",
    "gt_scene.estimate_normals(\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=50)\n",
    ")\n",
    "\n",
    "print('icp')\n",
    "result_icp = o3d.pipelines.registration.registration_colored_icp(\n",
    "    pred_scene, gt_scene, voxel_size * 10, np.eye(4),\n",
    "    o3d.pipelines.registration.TransformationEstimationForColoredICP(),\n",
    "    o3d.pipelines.registration.ICPConvergenceCriteria(\n",
    "        relative_fitness=1e-6,\n",
    "        relative_rmse=1e-6,\n",
    "        max_iteration=10000\n",
    "    )\n",
    ")\n",
    "\n",
    "np.round(result_icp.transformation, 2), result_icp.inlier_rmse, result_icp.fitness\n",
    "np.save('colored_icp_transformation.npy', result_icp.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scene.transform(result_icp.transformation)\n",
    "o3d.io.write_point_cloud(\"gt_scene_aligned.ply\", gt_scene)\n",
    "o3d.io.write_point_cloud(\"pred_scene_aligned.ply\", pred_scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 303663 points."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_scene = o3d.io.read_point_cloud('gt2.ply')\n",
    "pred_scene = o3d.io.read_point_cloud('pred2.ply')\n",
    "result_ransac = np.load('ransac_transformation.npy')\n",
    "result_icp = np.load('colored_icp_transformation.npy')\n",
    "\n",
    "pred_box = pred_scene.get_oriented_bounding_box()\n",
    "pred_scene.translate(-pred_box.center)\n",
    "pred_box = pred_scene.get_oriented_bounding_box()\n",
    "pred_scene.rotate(pred_box.R.T)\n",
    "# transform according ransac icp\n",
    "pred_scene.transform(result_ransac)\n",
    "pred_scene.transform(result_icp)\n",
    "\n",
    "gt_box = gt_scene.get_oriented_bounding_box()\n",
    "gt_scene.translate(-gt_box.center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_l1 = np.round(chamfer_distance_p(np.asarray(pred_scene.points), np.asarray(gt_scene.points)), 5)\n",
    "c_l2 = np.round(chamfer_distance_l2_o3d(np.asarray(pred_scene.points), np.asarray(gt_scene.points)), 5)\n",
    "f_score_1 = np.round(f_score_pcd(np.asarray(pred_scene.points), np.asarray(gt_scene.points), voxel_size * 1), 5)\n",
    "f_score_2 = np.round(f_score_pcd(np.asarray(pred_scene.points), np.asarray(gt_scene.points), voxel_size * 2), 5)\n",
    "f_score_3 = np.round(f_score_pcd(np.asarray(pred_scene.points), np.asarray(gt_scene.points), voxel_size * 3), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20644, 0.15114, 0.25313, 0.48242, 0.63633)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_l1, c_l2, f_score_1, f_score_2, f_score_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
